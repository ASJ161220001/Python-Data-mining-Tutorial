# Day13 : 语言模型

> **作者**：长行
>
> **时间**：2020.05.10

在了解了词典分词之后，我们发现一些类似于“商品和服务”的句子并不能被准确地识别。

由此，我们设想如何能够提高准确率？一个简单有效的方法就是通过人工置顶分词结果的优先级列表，并使用到分词器中。

但是这样的方法显然需要大量的人工成本，并不现实。所以我们考虑是否可以通过制作一个完成分词的语料库，通过统计所有切分方式的次数来实现优先级列表的计算。每个句子出现频率越高的切分方式，就拥有越高的权重。

## 语言模型

这样我们就引入了**语言模型**(Language Model,LM)的概念。

语言模型是语言现象的数学抽样，简单来说，语言模型就是计算给定的句子w出现概率p(w)的模型。

例如，我们使用何晗《自然语言处理入门》中作为案例的语料库：

```
商品 和 服务
商品 和服 物美价廉
服务 和 货币
```

根据语料库，因为样本空间大小为3，所以仅有的3个句子平分了所有的概率，即它们的概率都是1/3，其他所有句子的概率均为0。所以语言模型p(w)如下所示：

```
p(商品 和 服务) = 1/3
p(商品 和服 物美价廉) = 1/3
p(服务 和 货币) = 1/3
```

这个语言模型只能用来判断已有的这3个句子，而其他的句子因为概率都是0而无法判断。

一个方式就是增大样本数量，但是句子总数无穷无尽，无法枚举，即使是大型语料库，也只能“枚举”几百万个句子。因此，无论我们使用多达的语料库，绝大部分的句子还是在语料库之外，即它们的概率仍然为0，这个现象被称为**数据稀疏**。

由此，我们考虑到句子数虽然是无限的，但是句子中包含的词却是有限的。于是我们尝试从构成句子的角度去建模句子，即通过已经说出口的所有的词语，来预测下一个词语。

在预测的过程中，我们将句子的开头和结尾当做两个特殊的单词，分别记作BOS(Begin of Sentence)和EOS(End of Sentence)。

在当前算法下，句子“商品 和 服务“的概率的计算结果如下：

```
p(商品|BOS) = 2/3
p(和|BOS 商品) = 1/2
p(服务|BOS 商品 和) = 1/1
p(EOS|BOS 商品 和 服务) = 1/1
p(商品 和 服务) = p(商品|BOS) + p(和|BOS 商品) + p(服务|BOS 商品 和) + p(EOS|BOS 商品 和 服务) = 2/3 * 1/2 * 1/1 * 1/1 = 1/3
```

此时语言模型p(w)如下所示：

```
p(商品 和 服务) = 1/3
p(商品 和服 物美价廉) = 1/3
p(服务 和 货币) = 1/3
```

这样仍然只能识别语料库中的文本，语料库外的文本的概率仍然为0，存在数据稀疏的问题。

## 马尔可夫链

为了解决这个问题，我么引入**马尔科夫假设**：给定时间线上有一串事件顺序发生，假设每个事件的发生概率只取决于前一个事件，那么这串事件构成的因果链就被称为**马尔可夫链**。

基于此假设，每次计算只涉及两个单词的二元接续，此时的语言模型称为**二元语法模型**。

在当前算法下，句子“商品 和 服务“的概率的计算结果如下：

```
p(商品|BOS) = 2/3
p(和|商品) = 1/2
p(服务|和) = 1/2
p(EOS|服务) = 1/2
p(商品 和 服务) = p(商品|BOS) + p(和|BOS 商品) + p(服务|BOS 商品 和) + p(EOS|BOS 商品 和 服务) = 2/3 * 1/2 * 1/2 * 1/2 = 1/12
```

此时语言模型p(w)如下所示：

```
p(商品 和 服务) = 1/12 = 2/24
p(商品 和服 物美价廉) = 1/3 = 8/24
p(服务 和 货币) = 1/8 = 3/24
p(商品 和 货币) = 1/6 = 4/24
p(服务) = 1/6 = 4/24
p(服务 和 服务) = 1/24
p(商品 和 服务 和 ...) = 1/24
p(服务 和 服务 和 ...) = 1/24
```

可见，当前的语言模型不但可以识别语料库中的文本，也可以识别一部分的语料库外的文本了，缓解了一部分数据稀疏的问题。

## n元语法

利用类似的思路，我们得到n元语法的定义：每个单词的概率仅取决于该单词之前的n-1个单词。通常而言，实际工程中不会使用n≥4的算法。

另外，深度学习也剔除了一种递归神经网络语言模型(RNN Language Model)，理论上可以记忆无限个单词，可以看作“无穷元语法”。

## 数据稀疏与平滑策略

对于n元语法模型中，n越大，数据稀疏的问题就会越严峻。

因此，考虑到低阶n元语法更丰富，一个自然而然的解决方案就是用低阶n元语法去平滑高阶n元语法。就是避免高阶n元语法因为频次突然为0，而使用低阶的频次去平滑它。

即高阶n元语法和低阶n元语法同时占一定的比例，当高阶n元语法为0时，低阶n元语法不为0。

此时，新的二元语法的概率为：

```
p(Wa|Wb) = n * p(Wa|Wb) + (1-n) * p(Wa)
```

其中，n∈(0,1)，为常数平滑因子。

> 学习参考文献：《自然语言处理入门》(何晗)：3.1